# Poisoning Large Language Models


This project explores the concept of data poisoning attacks on large language models (LLMs), specifically targeting models used for sentiment analysis tasks. With the increasing reliance on LLMs across various sectors, the potential for malicious data manipulation poses a significant threat to their integrity and reliability. Our research focuses on demonstrating how subtly poisoned datasets can significantly impact the performance and decision-making process of LLMs. Through comprehensive analysis and experimentation with models like BERT, trained on datasets such as SST and IMDB, we aim to shed light on vulnerabilities and propose potential mitigation strategies to safeguard against such attacks.

